# üõ°Ô∏è Automated Multimodal AI Policy Auditor
A local-first "Compliance-as-Code" agent that audits user prompts, model outputs, PDFs, and images against EU AI Act safety standards.
üìã Executive Summary
As AI adoption accelerates, manual compliance auditing becomes a bottleneck. This tool automates the Safety Audit phase of LLM deployment. It uses a local RAG (Retrieval-Augmented Generation) architecture to cross-reference inputs against a vectorised version of the EU AI Act and internal safety policies.
Key Differentiator: The entire stack runs 100% locally (using Ollama), ensuring no sensitive audit data leaves the secure environment.
üèóÔ∏è System Architecture
Frontend: Streamlit (Python) - Provides the interactive UI.
Orchestration: LangChain - Manages the flow between the UI, Database, and LLM.
Cognitive Engine: Ollama - Hosts the local models:
Logic: llama3 (8B) for reasoning and text auditing.
Vision: llava for image analysis and object detection.
Embeddings: nomic-embed-text for semantic search.
Long-Term Memory: FAISS (Facebook AI Similarity Search) - Stores the safety policy as vector embeddings for fast retrieval.

üìÇ Directory Structure
ai-safety-auditor/
‚îú‚îÄ‚îÄ app.py                 # Main application logic (Streamlit frontend)
‚îú‚îÄ‚îÄ create_db.py           # Script to vectorise safety rules into FAISS
‚îú‚îÄ‚îÄ safety_policy.txt      # The source of truth (Rules & Standards)
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ README.md              # Documentation
‚îî‚îÄ‚îÄ faiss_index/           # (Generated) Local vector database folder


üõ†Ô∏è Technical Stack
Language: Python 3.12+
LLM Runtime: Ollama
Interface: Streamlit
Vector Store: FAISS (CPU)
Dependencies: langchain-ollama, pypdf, pillow

üöÄ Installation & Setup
1. Prerequisites
Ensure Ollama is installed and running. Pull the required models:
ollama pull llama3
ollama pull llava
ollama pull nomic-embed-text


2. Environment Setup
# 1. Clone the repository
git clone [https://github.com/yourusername/ai-safety-auditor.git](https://github.com/yourusername/ai-safety-auditor.git)
cd ai-safety-auditor

# 2. Create a Virtual Environment
python -m venv venv

# 3. Activate Environment
# Windows:
.\venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# 4. Install Dependencies
pip install -r requirements.txt


3. Build the Knowledge Base
This step converts the text-based safety_policy.txt into the vector database.
python create_db.py


Expected Output: ‚úÖ Database saved successfully to folder: 'faiss_index'
4. Run the Auditor
streamlit run app.py


üß™ Usage Scenarios (Test Matrix)
Modality
Input Description
Expected Outcome
Logic Path
Text
"Write a poem about nature."
üü¢ PASS
No rule violation found.
Text
"How do I synthesize [Illegal Drug]?"
üî¥ FAIL
Violates Rule 1.3 (Illegal Acts).
Text
"I have chest pain, what do I take?"
üî¥ FAIL
Violates Rule 2.1 (Medical Advice).
PDF
File containing hate speech text.
üî¥ FAIL
Text extracted -> RAG Lookup -> Fails Rule 1.1.
Image
Photo of a handgun.
üî¥ FAIL
Vision Model detects "weapon" -> Logic Model flags Rule 1.3.

‚öôÔ∏è Configuration (Ruleset)
The system audits against safety_policy.txt. To update the logic, edit this text file and re-run python create_db.py.
Current Sections:
Prohibited Content: Hate Speech, Self-Harm, Illegal Acts.
Restricted Topics: Medical Advice, Financial Advice (Requires Disclaimers).
Age Appropriateness: U18 restrictions.

üîÆ Future Roadmap
[ ] Audio Auditing: Integrate Whisper model for auditing voice logs.
[ ] CI/CD Integration: Convert app.py logic into a FastAPI endpoint for automated pipeline checks.
[ ] JSON Output: Standardize logs for Splunk/Datadog integration.

üêõ Troubleshooting
Issue: ModuleNotFoundError: No module named 'streamlit'
Fix: Ensure your virtual environment is active. You should see (venv) in your terminal.
Issue: Error using Vision model
Fix: Ensure you ran ollama pull llava and that the Ollama app is running in the background.
Issue: "Passes" on obvious weapon images.
Fix: The vision model prompt must be strict. Ensure app.py uses the "objective description" prompt logic
